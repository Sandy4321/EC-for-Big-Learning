{"name":"EC for Big Learning","tagline":"Join us in fusing our EC classifiers","body":"# Big Data Learning\r\n\r\nHave you ever wanted to run your EC algorithm in the cloud? Discouraged by the complexity of EC2? We will deploy your EC algorithm on the cloud for you with our FCUBE framework!\r\n\r\nFCUBE supports a Bring Your Own Learner (BYOL) model: it deploys your EC algorithm to hundreds of machines and does all the data management for you. No scripts, no launch hassles, no tedious result collection. FCUBE is (EC) deployment as a service:\r\n\r\n![FCUBE](images/f3.png)\r\n\r\nFor this competition, our goal is to unite the developers of interesting EC classifier algorithms. We seek an experienced informed discussion on the various approaches and techniques without being distracted by one problem at hand. Therefore, we have set up the following format:\r\n\r\n* Everyone gets the same computational budget in Amazon EC2\r\n* Everyone works on the same datasets\r\n* Organizers select the features\r\n* You contribute a classifier learning algorithm with your own fitness function, operators and search logic\r\n* You contribute a classifier learning algorithm which accepts training data in csv format and references a Java properties file which you provide (details below), and outputs a classifier.\r\n* You contribute a classifier learning algorithm in executable format (Java, python) or as source code (must be compilable in Linux: C, C++ etc)\r\n* You contribute a piece of code which applies your classifier to test data and produces labels.\r\n* FCUBE executes your algorithm with the competition training data\r\n* FCUBE retrieves the solutions from the cloud nodes, computes the testing predictions, and returns them to you.\r\n* FCUBE also filters and fuses the predictions using different methods. Everyone receives their fused results and everyone contributes to a collaborative fused solution among all contributors.\r\n\r\n# Instructions for competitors.\r\n\r\nThe competition will be divided in three phases:\r\n\r\n## Phase 1: Bring Your Own Learner\r\nIn a first step, participants will need to adapt their learners to be compliant with the two following interfaces:\r\n\r\n###Learner Training interface\r\n\r\n![Predict Interface](images/learnerbbtrain.png)\r\n\r\n####Inputs:\r\n1. a path to a CSV file\r\n2. learning time deadline\r\n3. Properties File with Java syntax (for your extra parameters)\r\n\r\n####Outputs:\r\n1. a model stored in a single file on disk\r\n\r\n\r\n###Learner Predict interface\r\n\r\n![Predict Interface](images/learnerbbpredict.png)\r\n\r\n####Inputs:\r\n1. path to a CSV file\r\n2. path to where the model is stored\r\n3. path to where the predictions will be stored\r\n\r\n####Outputs:\r\n1. predictions in CSV file (one label per line)\r\n\r\n\r\n### The data format</h1>\r\nIn this edition of the workshop, we will target binary classification problems. Algorithms submitted to the competition must read data in CSV format where the last column contains the label (0 or 1). For ease of use, we support two csv formats:\r\n\r\n1. csv without header line\r\n2. csv with a header line: the first line of the file contains the name and type of the features (integer, float, or nominal). \r\n\r\nPlease find below an example of data file with header composed of 5 features plus the label. In the example variable X1 is integer. X2 and X3 are floats. Finally X4 are X5 are nominal variables.\r\n<pre><code>$ head -n 5 example.csv\r\ni_X1,f_X2,f_X3,n_X4,n_X5,label\r\n2,0.7879572,1.0466304,blue,c,0.0\r\n6,0.58576685,0.20243178,yellow,b,0.0\r\n18,0.7604114,0.8667035,green,a,1.0\r\n25,0.7485672,1.3335116,blue,b,1.0\r\n</code></pre>\r\n\r\n\r\n## Phase 2: Choose the deployment strategy:\r\n\r\nOnce we have the final number of participants, each participant will be assigned a budget in Amazon EC2. Then, each participant will be asked to choose a combination of:\r\n\r\n1. EC2 flavor, i.e the virtual machine specs (check EC2 Instance Type Details [here](https://aws.amazon.com/ec2/instance-types/))\r\n2. running time per instance\r\n3. number of instances\r\n\r\nParticipants will also need to decide a set of parameters concerning the data-parallel strategy:\r\n\r\n1. % of data sampled per instance\r\n2. % of variables/features sampled per instance \r\n\r\nIn the last step prior to deployment, participants will have the option to expose a range of possible choices for their learner-specific parameters. This way, it will possible to assign different parameters to the different instances running on the cloud. More details to come on this aspect.\r\n\r\n## Phase 3: Deployment of the learners and communication of the results\r\nIn the last phase we will deploy your learners in EC2. We will analyze the predictions of your learner and communicate performance metrics.\r\n\r\n# The datasets:\r\n\r\nFor each dataset, we will release samples of different sizes. These samples will allow to estimate the running time of the classifier learning algorithms given the size of the data.\r\n### Higgs dataset: \r\n\r\nPlease remain connected for the updates on the datasets used for the competition.\r\n\r\n# Example of learner: GPFunction</h1>\r\n\r\nIn this section we provide an example of a learner integrated in FCUBE, namely our GPFunction classifier  learning algorithm based on Genetic Programming. Please download the GPFunction from [here](https://github.com/flexgp/EC-for-Big-Learning/releases).\r\n\r\nWe use the split higgs_02.csv released to prepare the competition to demonstrate the interfaces of the GPFunction classifier. The Higgs dataset is composed of 28 features and a binary label 0 or 1. Note that in this case the 28 features are floats. You will realize that the released split contains only 5 features. Since FCUBE will automatically sample exemplars and features for you, your learner should be able to work with any number of features (so avoid hard-coded values!).\r\n\r\n###Step 0 (optional): prepare a properties file in Java syntax\r\nFirst we prepare a properties file in Java syntax for the parameters specific to GP Function classifier:\r\n<pre><code>$ nano params.properties\r\nxover_op=operator.SinglePointKozaCrossover\r\nexternal_threads=4\r\nfalse_negative_weight=0.5\r\npop_size=500\r\n</code></pre>\r\n\r\n###Step 2: execute the classifier learning algorithm\r\nThe GPFunction learner is executed given a csv file, a time deadline, and a properties file in Java Syntax. In the example below, the GPFunction classifier is invoked with a time deadline of 10 minutes and the properties file shown above.\r\n<pre><code>$ java -jar gpfunction.jar -train split_02.csv -minutes 10 -properties params.properties\r\n</code></pre>\r\n\r\n###Step 3: obtain the predictions\r\nThe GPFunction learner produces several models. Let is pick the model called mostAccurate.txt and generate predictions for the same split split_02.csv.\r\n<pre><code>$ java -jar gpfunction.jar -predict split_02.csv -model mostAccurate.txt -o predictions.csv\r\n</code></pre> \r\nThe executable gpfunction.jar will generate a csv file named predictions.csv containing one label per line.\r\n\r\n\r\n### Support or Contact\r\nHaving with the adaptation of your learner? Feel free to contact us by email at [iarnaldo@mit.edu](mailto:iarnaldo@mit.edu) and weâ€™ll try to help you sort it out.\r\n\r\n### Authors and Contributors\r\nThis competition is organized by the [Any-Scale Learning For All (ALFA)](http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/) group at MIT.\r\n![ALFA](images/ALFA-logo-lousy.png)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}