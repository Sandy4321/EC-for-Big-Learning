<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>EC for Big Learning by flexgp</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/main.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1>EC for Big Learning</h1>
        <p>Join us in fusing our EC classifiers</p>
      </header>

      <div id="banner">
        <span id="logo"></span>

        <a href="https://github.com/flexgp/EC-for-Big-Learning" class="button fork"><strong>View On GitHub</strong></a>
        <div class="downloads">
          <span>Downloads:</span>
          <ul>
            <li><a href="https://github.com/flexgp/EC-for-Big-Learning/zipball/master" class="button">ZIP</a></li>
            <li><a href="https://github.com/flexgp/EC-for-Big-Learning/tarball/master" class="button">TAR</a></li>
          </ul>
        </div>
      </div><!-- end banner -->

    <div class="wrapper">
      <nav>
        <ul></ul>
      </nav>
      <section>
        <h1>
<a name="big-data-learning" class="anchor" href="#big-data-learning"><span class="octicon octicon-link"></span></a>Big Data Learning</h1>

<p>Have you ever wanted to run your EC algorithm in the cloud? Discouraged by the complexity of EC2? We will deploy your EC algorithm on the cloud for you with our FCUBE framework!</p>

<p>FCUBE supports a Bring Your Own Learner (BYOL) model: it deploys your EC algorithm to hundreds of machines and does all the data management for you. No scripts, no launch hassles, no tedious result collection. FCUBE is (EC) deployment as a service:</p>

<p><img src="images/f3.png" alt="FCUBE"></p>

<p>For this competition, our goal is to unite the developers of interesting EC classifier algorithms to solve relevant problems of public domain. We seek an experienced informed discussion on the various approaches and techniques without being distracted by one problem at hand. Therefore, we have set up the following format:</p>

<ul>
<li>Everyone gets the same computational budget in Amazon EC2</li>
<li>Everyone works on the same datasets</li>
<li>Organizers select the features</li>
<li>You contribute a classifier learning algorithm with your own fitness function, operators and search logic</li>
<li>You contribute a classifier learning algorithm which accepts training data in csv format and references a Java properties file which you provide (details below), and outputs a classifier.</li>
<li>You contribute a classifier learning algorithm in executable format (Java, python) or as source code (must be compilable in Linux: C, C++ etc)</li>
<li>You contribute a piece of code which applies your classifier to test data and produces labels.</li>
<li>FCUBE executes your algorithm with the competition training data</li>
<li>FCUBE retrieves the solutions from the cloud nodes, computes the testing predictions, and returns them to you.</li>
<li>FCUBE also filters and fuses the predictions using different methods. Everyone receives their fused results and everyone contributes to a collaborative fused solution among all contributors.</li>
</ul><h1>
<a name="instructions-for-competitors" class="anchor" href="#instructions-for-competitors"><span class="octicon octicon-link"></span></a>Instructions for competitors.</h1>

<p>The competition will be divided in three phases:</p>

<h2>
<a name="phase-1-bring-your-own-learner" class="anchor" href="#phase-1-bring-your-own-learner"><span class="octicon octicon-link"></span></a>Phase 1: Bring Your Own Learner</h2>

<p>In a first step, you will adapt your learner to be compliant with FCUBE's interface; we provide an example with <a href="gpfunction.jar" target="_blank">GPFunction</a> (one of our Java learners) together with a dataset you can use to debug yours:</p>

<h3>
<a name="learner-training-interface" class="anchor" href="#learner-training-interface"><span class="octicon octicon-link"></span></a>Learner Training interface</h3>

<p><img src="images/learnerbbtrain.png" alt="Predict Interface"></p>

<h4>
<a name="inputs" class="anchor" href="#inputs"><span class="octicon octicon-link"></span></a>Inputs:</h4>

<ol>
<li>a path to a CSV file: we support csv files with and without headers. When headers are included, the first line of the file contains the name and type of the features (integer, float, or nominal). Check this dummy dataset <a href="dummy_headers.csv" target="_blank">with headers</a> and <a href="dummy_noheaders.csv" target="_blank">without headers</a> and this split of the Higgs dataset <a href="higgs_header_02.csv" target="_blank">with headers</a> and <a href="higgs_noheader_02.csv" target="_blank">without headers</a>.</li>
<li>learning time deadline</li>
<li>Properties File with Java syntax for your extra parameters. Check this <a href="parameters.properties" target="_blank">example</a>.</li>
</ol><h4>
<a name="outputs" class="anchor" href="#outputs"><span class="octicon octicon-link"></span></a>Outputs:</h4>

<ol>
<li>a model stored in a single file on disk</li>
</ol><p>Example with our <a href="gpfunction.jar" target="_blank">GPFunction</a> learner:</p>

<pre><code>$ java -jar gpfunction.jar -train higgs_noheader_02.csv -minutes 10 -properties params.properties
</code></pre>

<h3>
<a name="learner-predict-interface" class="anchor" href="#learner-predict-interface"><span class="octicon octicon-link"></span></a>Learner Predict interface</h3>

<p><img src="images/learnerbbpredict.png" alt="Predict Interface"></p>

<h4>
<a name="inputs-1" class="anchor" href="#inputs-1"><span class="octicon octicon-link"></span></a>Inputs:</h4>

<ol>
<li>path to a CSV file</li>
<li>path to where the model is stored</li>
<li>path to where the predictions will be stored</li>
</ol><h4>
<a name="outputs-1" class="anchor" href="#outputs-1"><span class="octicon octicon-link"></span></a>Outputs:</h4>

<ol>
<li>predictions in CSV file (one label per line)</li>
</ol><p>Example: the GPFunction learner produces several models. Let is pick the model called mostAccurate.txt and generate predictions for the same split higgs_noheader_02.csv.</p>

<pre><code>$ java -jar gpfunction.jar -predict higgs_noheader_02.csv -model mostAccurate.txt -o predictions.csv
</code></pre> 

<p>The executable gpfunction.jar will generate a csv file named predictions.csv containing one label per line.</p>

<h2>
<a name="phase-2-choose-the-deployment-strategy" class="anchor" href="#phase-2-choose-the-deployment-strategy"><span class="octicon octicon-link"></span></a>Phase 2: Choose the deployment strategy:</h2>

<p>Once we have the final number of participants, each participant will be assigned a budget in Amazon EC2. Then, each participant will be asked to choose a combination of:</p>

<ol>
<li>EC2 flavor, i.e the virtual machine specs (check EC2 Instance Type Details <a href="https://aws.amazon.com/ec2/instance-types/">here</a>)</li>
<li>running time per instance</li>
<li>number of instances</li>
<li>data-parallel strategy (% of data and % of variables/features sampled per instance)</li>
</ol><p>In the last step prior to deployment, participants will have the option to expose a range of possible choices for their learner-specific parameters. This way, it will possible to assign different parameters to the different instances running on the cloud. More details to come on this aspect.</p>

<p>Finally, we will deploy your learners in EC2. We will analyze the predictions of your learner and communicate performance metrics.</p>

<h1>
<a name="the-datasets" class="anchor" href="#the-datasets"><span class="octicon octicon-link"></span></a>The datasets:</h1>

<p>In this edition of the workshop, we will target binary classification problems. For each dataset, we will release samples of different sizes. These samples will allow to estimate the running time of the classifier learning algorithms given the size of the data.</p>

<h2>
<a name="higgs-dataset-the-higgs-dataset-is-a-public-dataset-it-is-composed-of-11000000-exemplars-and-28-real-valued-features-expect-a-csv-file-with-any-number-of-features-where-the-last-column-is-the-label-0-or-1" class="anchor" href="#higgs-dataset-the-higgs-dataset-is-a-public-dataset-it-is-composed-of-11000000-exemplars-and-28-real-valued-features-expect-a-csv-file-with-any-number-of-features-where-the-last-column-is-the-label-0-or-1"><span class="octicon octicon-link"></span></a>Higgs dataset: The Higgs dataset is a public dataset. It is composed of 11000000 exemplars and 28 real-valued features. Expect a CSV file with any number of features where the last column is the label (0 or 1).</h2>

<p>Please remain connected for updates on the datasets.</p>

<h1>
<a name="support-or-contact" class="anchor" href="#support-or-contact"><span class="octicon octicon-link"></span></a>Support or Contact</h1>

<p>Having with the adaptation of your learner? Feel free to contact us by email at <a href="mailto:iarnaldo@mit.edu">iarnaldo@mit.edu</a> and weâ€™ll try to help you sort it out.</p>

<h1>
<a name="authors-and-contributors" class="anchor" href="#authors-and-contributors"><span class="octicon octicon-link"></span></a>Authors and Contributors</h1>

<p>This competition is organized by the <a href="http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/">Any-Scale Learning For All (ALFA)</a> group at MIT.
<img src="images/ALFA-logo-lousy.png" alt="ALFA"></p>
      </section>
      <footer>
        <p>Project maintained by <a href="https://github.com/flexgp">flexgp</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>